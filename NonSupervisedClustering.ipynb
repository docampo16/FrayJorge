{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyNTuLKoRUzkxTcQVYKsI1Cb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<a name=\"top\"></a>\n","# A land use classification for Fray Jorge\n","Developed by Diego Ocampo Melgar\n","\n","[Contact](diego.ocampo.melgar@gmail.com)"],"metadata":{"id":"F5PLZMLxT9gj"}},{"cell_type":"markdown","source":["## Index\n","\n","[Dependencias](#setup)\n","\n","[Configuración](#config)\n","\n","[Loop de entrenamiento por clases](#loop)\n","\n","[Integrar datos](#join)\n","\n","[Limpieza de datos](#clean)\n","\n","[Diseño del modelo FNN](#model)\n","\n","[Entrenamiento del modelo](#train)\n","\n","[Desempeño del modelo](#performance)\n","\n","[Predicción](#predict)\n","\n","[Visualización](#view)\n"],"metadata":{"id":"W-6S6UOlUM4A"}},{"cell_type":"markdown","source":["[Fuente](https://chat.deepseek.com/a/chat/s/b31ae746-5a2a-4a17-800f-2c162618c5e6)"],"metadata":{"id":"E-mTsoY3ySY-"}},{"cell_type":"markdown","source":["# Dependencies"],"metadata":{"id":"ZLZ2AVg62asJ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UA8F-YJm2hh2","executionInfo":{"status":"ok","timestamp":1747249504646,"user_tz":240,"elapsed":42815,"user":{"displayName":"Diego Ocampo Melgar","userId":"04262832703474159132"}},"outputId":"ef47c745-d6bb-4e73-ddb2-76f3b2cc31ad"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install rasterio rioxarray"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TQVRASGH2aJr","executionInfo":{"status":"ok","timestamp":1747249508457,"user_tz":240,"elapsed":3809,"user":{"displayName":"Diego Ocampo Melgar","userId":"04262832703474159132"}},"outputId":"79d10850-c2c4-4ff6-96c0-25c9f4515962"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rasterio\n","  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n","Collecting rioxarray\n","  Downloading rioxarray-0.19.0-py3-none-any.whl.metadata (5.5 kB)\n","Collecting affine (from rasterio)\n","  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.4.26)\n","Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.1.8)\n","Collecting cligj>=0.5 (from rasterio)\n","  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n","Collecting click-plugins (from rasterio)\n","  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from rioxarray) (24.2)\n","Requirement already satisfied: xarray>=2024.7.0 in /usr/local/lib/python3.11/dist-packages (from rioxarray) (2025.3.1)\n","Requirement already satisfied: pyproj>=3.3 in /usr/local/lib/python3.11/dist-packages (from rioxarray) (3.7.1)\n","Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from xarray>=2024.7.0->rioxarray) (2.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray>=2024.7.0->rioxarray) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray>=2024.7.0->rioxarray) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray>=2024.7.0->rioxarray) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray>=2024.7.0->rioxarray) (1.17.0)\n","Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rioxarray-0.19.0-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n","Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n","Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n","Installing collected packages: cligj, click-plugins, affine, rasterio, rioxarray\n","Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.4.3 rioxarray-0.19.0\n"]}]},{"cell_type":"code","source":["\n","data_folder = \"/content/drive/MyDrive/Programming/Colab Notebooks/Fray Jorge LULC/FrayJorge/input/S2/Sentinel2_Exports/processed\"\n","output_folder = \"/content/drive/MyDrive/Programming/Colab Notebooks/Fray Jorge LULC/FrayJorge/output\"\n","band_names = (\n","  ['B2', 'B3', 'B4', 'B8'] +\n","  ['B5', 'B6', 'B7', 'B8A', 'B11', 'B12'] +\n","  ['B1', 'B9'] +\n","  ['SCL']\n",")"],"metadata":{"id":"8Zu6GWhm2oJX","executionInfo":{"status":"ok","timestamp":1747249508482,"user_tz":240,"elapsed":3,"user":{"displayName":"Diego Ocampo Melgar","userId":"04262832703474159132"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\n","\n","import os\n","import rasterio\n","import rioxarray\n","\n","# Get the list of files in the data folder\n","file_list = os.listdir(data_folder)\n","\n","# Filter for files that are likely raster files (e.g., with .tif extension)\n","raster_files = [f for f in file_list if f.endswith('.tif')]\n","\n","if not raster_files:\n","  print(\"No raster files found in the data folder.\")\n","else:\n","  # Get the first raster file in the list\n","  first_raster_file = raster_files[0]\n","  first_raster_path = os.path.join(data_folder, first_raster_file)\n","\n","\n","  # Load the raster with band dimension\n","  da = rioxarray.open_rasterio(first_raster_path, masked=True)\n","  band_names = list(da.attrs['long_name'])  # e.g., ['B2', 'B3', ..., 'SCL']\n","  da = da.assign_coords(band=list(da.attrs['long_name']))\n","  ds = da.to_dataset(dim='band')\n","  ds = ds.expand_dims(time=[\"2024-03-08\"])\n",""],"metadata":{"id":"khvJq93y2svw","executionInfo":{"status":"ok","timestamp":1747249543515,"user_tz":240,"elapsed":17589,"user":{"displayName":"Diego Ocampo Melgar","userId":"04262832703474159132"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","def compute_indices(data):\n","\n","    # Vegetation Indices\n","    data[\"NDVI\"] = (data.nir - data.red) / (data.nir + data.red)\n","    #data[\"MNDVI\"] = (data.rededge1 - data.red) / (data.rededge1 + data.red)\n","    #data[\"EVI\"] = 2.5 * (data.nir - data.red) / (data.nir + (6.0*data.red) - (7.5*data.blue)+ 1)\n","    #data[\"MSAVI\"] = (2 * data.nir + 1 - np.sqrt((2 * data.nir + 1)**2 - 8 * (data.nir - data.red))) / 2\n","    data[\"SAVI\"] = (data.nir - data.red) * (1.5) / (data.nir + data.red + 0.5)\n","    data[\"IAVI\"] = (data.nir - (data.red - ((1.12 + 0.65) / 2)*(data.blue - data.red))) / (data.nir + (data.red - ((1.12 + 0.65) / 2)*(data.blue - data.red)))\n","    data[\"OSAVI\"] = ((1 + 0.16)*(data.nir - data.red)) / (data.nir + data.swir16 + 0.61)\n","    #sigma = 0.5*(data['nir'] + data['red'])\n","    #knr = np.exp(-(data['nir'] - data['red'])**2/(2*sigma**2))\n","    #data['kNDVI'] = (1-knr) / (1+knr)\n","\n","    # Water Indices\n","    data[\"NDWI\"] = (data.green - data.nir) / (data.green + data.nir + 1e-6)\n","    data[\"MNDWI\"] = (data.green - data.swir16) / (data.green + data.swir16 + 1e-6)\n","    data['ANDWI'] = (data['blue']+data['green']+data['red']-data['nir']-data['swir16']-data['swir22']) / (data['blue']+data['green']+data['red']+data['nir']+data['swir16']+data['swir22'])\n","    data[\"NDMI\"] = (data.nir - data.swir16) / (data.nir  + data.swir16 + 1e-6)\n","    data[\"NMDI\"] = (data.nir  - ( data.swir16 - data.rededge2 ) ) / ( data.nir  + ( data.swir16 - data.swir22) )\n","\n","    # Bare Soil Indices\n","    data[\"BSI\"] = ((data.swir16 + data.red) - (data.nir + data.blue)) / ((data.swir16 + data.red) + (data.nir + data.blue))\n","    #data[\"mSBI\"] = (data.swir16 + data.swir22) / 2\n","    data[\"MSBI\"] = (0.406*data.green) + (0.60*data.red) + (0.645*data.rededge2) + (0.243*data.nir09) #https://www.indexdatabase.de/db/i-single.php?id=570\n","\n","    # Tasseled Cap Components\n","    data['Brightness'] = 0.3510*data['blue'] + 0.3813*data['green'] + 0.3437*data['red'] + 0.7196*data['nir'] + 0.2396*data['swir16'] + 0.1949*data['swir22']\n","    data['Greeness'] = -0.3599*data['blue'] - 0.3533*data['green'] - 0.4734*data['red'] + 0.6633*data['nir'] + 0.0087*data['swir16'] - 0.2856*data['swir22']\n","    data['Wetness'] = 0.2578*data['blue'] + 0.2305*data['green'] + 0.0883*data['red'] + 0.1071*data['nir'] - 0.7611*data['swir16'] - 0.5308*data['swir22']\n","\n","    # Urban/Built-up Indices\n","    #data[\"NDBI\"] = (data.swir16 - data.nir) / (data.swir16 + data.nir + 1e-6)\n","    #data[\"BU\"] = (data.swir22 - data.nir) / (data.swir22 + data.nir + 1e-6)\n","    #data[\"IBI\"] = (data[\"NDBI\"] - (data[\"NDVI\"] + data[\"NDWI\"])) / (data[\"NDBI\"] + (data[\"NDVI\"] + data[\"NDWI\"]))\n","    #data[\"UI\"] = (data.swir16 - data.red) / (data.swir16 + data.red + 1e-6)\n","    data[\"EBBI\"] = (data.swir16 - data.nir) / (10 * np.sqrt(data.red))\n","    #data[\"RRI\"] = data.blue + data.red - (2*data.green)\n","    ## https://www.mdpi.com/2071-1050/15/12/9704\n","    data[\"BCI\"] = (((data['Brightness'] + data['Wetness'])/2) - data['Greeness'])/(((data['Brightness'] - data['Wetness'])/2) + data['Greeness'])  # Biophysical composition index (BCI)\n","    data[\"BLFEI\"] = (((data.green + data.red + data.swir22)/3) - data.swir16)/(((data.green + data.red + data.swir22)/3) + data.swir16)  # Built-up land features extraction index (BLFEI)\n","    ## Artificial Surface Index\n","    ## https://www.mdpi.com/2072-4292/16/7/1126\n","    #MF = ((data.blue + data.green)-(data.nir + data.swir16))/((data.blue + data.green)+(data.nir + data.swir16))\n","    #AF = (data.nir - data.blue)/(data.nir + data.blue)\n","    #VSF = 1 - (data['NDVI'])*(data['MSAVI'])\n","    #MBI = ((data.swir16 - data.swir22 - data.nir)/(data.swir16 + data.swir22 + data.nir)) + 0.5\n","    #EMBI = (MBI - data[\"MNDWI\"] - 0.5)/(MBI + data[\"MNDWI\"] + 1.5)\n","    #SSF = 1- EMBI\n","    #data[\"ASI\"] = AF * MF * VSF * SSF\n","\n","    return data"],"metadata":{"id":"b_eNfYWOlNPt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Unsupervised Classification of Raster Stacks V1\n"],"metadata":{"id":"fb8w2CW4tSZZ"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"5dzGnQxvs59Q","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"error","timestamp":1747256091881,"user_tz":240,"elapsed":6505984,"user":{"displayName":"Diego Ocampo Melgar","userId":"04262832703474159132"}},"outputId":"1db0235c-ce6c-4b0d-e4b4-6cc917471ee0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running KMeans (k=5)...\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-6a6df2d98809>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         metrics = {\n\u001b[1;32m     63\u001b[0m             \u001b[0;34m'Model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;34m'Silhouette'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;34m'Calinski-Harabasz'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcalinski_harabasz_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;34m'Davies-Bouldin'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdavies_bouldin_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0m_silhouette_reduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_freqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_freqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     )\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpairwise_distances_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minter_clust_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintra_clust_dists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduce_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m             \u001b[0mchunk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2261\u001b[0;31m             \u001b[0mD_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2262\u001b[0m             \u001b[0m_check_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36m_silhouette_reduce\u001b[0;34m(D_chunk, start, labels, label_freqs)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0msample_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             cluster_distances[i] += np.bincount(\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0msample_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_freqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import numpy as np\n","import xarray as xr\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import (KMeans, MiniBatchKMeans, DBSCAN,\n","                           AgglomerativeClustering, SpectralClustering)\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","import seaborn as sns\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Load your raster stack (example structure)\n","# ds = xr.open_dataset('your_raster_stack.nc')\n","# Let's assume ds has dimensions (time, y, x) and multiple variables\n","\n","# 1. Data Preparation\n","def prepare_data(ds):\n","    \"\"\"Convert xarray Dataset to 2D array for clustering\"\"\"\n","    # Stack all variables and time steps\n","    data = ds.to_array().values  # (variables, time, y, x)\n","    # Reshape to (n_samples, n_features)\n","    n_samples = data.shape[2] * data.shape[3]  # y * x\n","    n_features = data.shape[0] * data.shape[1]  # variables * time\n","    return data.reshape(n_features, -1).T  # (n_samples, n_features)\n","\n","X = prepare_data(ds)\n","\n","# 2. Dimensionality Reduction (optional but recommended)\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","pca = PCA(n_components=0.95)  # Keep 95% variance\n","X_pca = pca.fit_transform(X_scaled)\n","\n","# 3. Define Clustering Models to Evaluate\n","models = {\n","    'KMeans (k=5)': KMeans(n_clusters=5, random_state=42),\n","    'MiniBatchKMeans (k=5)': MiniBatchKMeans(n_clusters=5, random_state=42),\n","    'DBSCAN (eps=0.5)': DBSCAN(eps=0.5, min_samples=5),\n","    'Agglomerative (k=5)': AgglomerativeClustering(n_clusters=5),\n","    'Spectral (k=5)': SpectralClustering(n_clusters=5, random_state=42)\n","}\n","\n","# 4. Evaluate Models\n","results = []\n","cluster_maps = {}\n","\n","for name, model in models.items():\n","    print(f\"Running {name}...\")\n","\n","    # Fit model\n","    labels = model.fit_predict(X_pca if 'Spectral' not in name else X_scaled)\n","\n","    # Skip metrics for DBSCAN if too many noise points\n","    if isinstance(model, DBSCAN) and (labels == -1).mean() > 0.5:\n","        print(f\"Skipping {name} - too many noise points\")\n","        continue\n","\n","    # Calculate metrics (only if not all noise)\n","    if len(np.unique(labels)) > 1:\n","        metrics = {\n","            'Model': name,\n","            'Silhouette': silhouette_score(X_pca, labels),\n","            'Calinski-Harabasz': calinski_harabasz_score(X_pca, labels),\n","            'Davies-Bouldin': davies_bouldin_score(X_pca, labels),\n","            'n_clusters': len(np.unique(labels))\n","        }\n","        results.append(metrics)\n","\n","    # Store labels for visualization\n","    cluster_maps[name] = labels.reshape(ds.dims['y'], ds.dims['x'])\n","\n","# 5. Performance Comparison\n","results_df = pd.DataFrame(results).set_index('Model')\n","print(\"\\nPerformance Metrics:\")\n","print(results_df)\n","\n","# Plot metrics\n","fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","for i, metric in enumerate(['Silhouette', 'Calinski-Harabasz', 'Davies-Bouldin']):\n","    sns.barplot(data=results_df.reset_index(), x='Model', y=metric, ax=axes[i])\n","    axes[i].set_title(metric)\n","    axes[i].tick_params(axis='x', rotation=45)\n","plt.tight_layout()\n","plt.show()\n","\n","# 6. Visualize Cluster Maps\n","n_models = len(cluster_maps)\n","fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 5))\n","if n_models == 1:\n","    axes = [axes]\n","\n","for ax, (name, labels) in zip(axes, cluster_maps.items()):\n","    im = ax.imshow(labels, cmap='viridis')\n","    ax.set_title(name)\n","    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","plt.tight_layout()\n","plt.show()\n","\n","# 7. Optimal Cluster Number Analysis (for KMeans)\n","if 'KMeans (k=5)' in models:\n","    silhouette_scores = []\n","    calinski_scores = []\n","    k_values = range(2, 11)\n","\n","    for k in k_values:\n","        kmeans = KMeans(n_clusters=k, random_state=42)\n","        labels = kmeans.fit_predict(X_pca)\n","        silhouette_scores.append(silhouette_score(X_pca, labels))\n","        calinski_scores.append(calinski_harabasz_score(X_pca, labels))\n","\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n","    ax1.plot(k_values, silhouette_scores, 'bo-')\n","    ax1.set_xlabel('Number of clusters')\n","    ax1.set_ylabel('Silhouette Score')\n","    ax1.set_title('Silhouette Method')\n","\n","    ax2.plot(k_values, calinski_scores, 'go-')\n","    ax2.set_xlabel('Number of clusters')\n","    ax2.set_ylabel('Calinski-Harabasz Score')\n","    ax2.set_title('Variance Ratio Method')\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","source":["Key Components:\n","* Data Preparation:\n","\n","  - Converts xarray Dataset to 2D array (pixels × features)\n","  - Handles multiple variables and time steps\n","\n","* Dimensionality Reduction:\n","\n","  - Standard scaling for normalization\n","\n","  - PCA to reduce computational complexity\n","\n","* Model Architectures:\n","\n","  - KMeans: Classic centroid-based clustering\n","\n","  - MiniBatchKMeans: Faster version for large datasets\n","\n","  - DBSCAN: Density-based clustering\n","\n","  - Agglomerative: Hierarchical clustering\n","\n","  - Spectral: Graph-based clustering\n","\n","* Performance Metrics:\n","\n","  - Silhouette Score (-1 to 1, higher is better)\n","\n","  - Calinski-Harabasz (higher is better)\n","\n","  - Davies-Bouldin (lower is better)\n","\n","* Visualization:\n","\n","  - Cluster maps for spatial patterns\n","\n","  - Metric comparison bar plots\n","\n","  - Elbow plots for optimal cluster number"],"metadata":{"id":"s1X1XqjjtoQL"}},{"cell_type":"markdown","source":["##Alternative Model Options:\n"],"metadata":{"id":"SyEFXvCpuV9S"}},{"cell_type":"markdown","source":["\n","\n","1.   Gaussian Mixture Models:\n","\n"],"metadata":{"id":"JbvMudI7uYJF"}},{"cell_type":"code","source":["from sklearn.mixture import GaussianMixture\n","models['GMM (k=5)'] = GaussianMixture(n_components=5, random_state=42)"],"metadata":{"id":"sV37T3shtoiA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. HDBSCAN (improved density-based):"],"metadata":{"id":"NqwsgdYkue4X"}},{"cell_type":"code","source":["from hdbscan import HDBSCAN\n","models['HDBSCAN'] = HDBSCAN(min_cluster_size=5)"],"metadata":{"id":"em5IdW4zut-N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Self-Organizing Maps:"],"metadata":{"id":"hmcy8ZTvukSj"}},{"cell_type":"code","source":["from minisom import MiniSom\n","som = MiniSom(5, 5, X_pca.shape[1], sigma=0.5, learning_rate=0.5)\n","som.train_random(X_pca, 100)"],"metadata":{"id":"m2s76p_2usBm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Performance Optimization Tips:\n","* For large rasters:\n","\n","  - Use MiniBatchKMeans or subsample\n","\n","  - Reduce PCA components\n","\n","  - Process in chunks\n","\n","* For better spatial coherence:\n","\n","  - Include spatial coordinates as features\n","\n","  - Use SpatialKMeans from sklearn-extensions\n","\n","* For temporal patterns:\n","\n","  - Extract temporal features before clustering\n","\n","  - Use TimeSeriesKMeans from tslearn\n","\n","This framework provides a complete pipeline from data preparation to model evaluation, with multiple visualization options to interpret the results. The choice of best model depends on your specific data characteristics and application requirements."],"metadata":{"id":"zE-4YFhvunfJ"}},{"cell_type":"markdown","source":["## Unsupervised Model Comparison Summary"],"metadata":{"id":"x1q67EJ_vqS3"}},{"cell_type":"markdown","source":["# Unsupervised Classification Models Comparison\n","\n","| Model               | Pros                                                                 | Cons                                                                 | Best For                                  |\n","|---------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|-------------------------------------------|\n","| **KMeans**          | ✅ Fast computation<br>✅ Scalable to large data<br>✅ Guaranteed convergence | ❌ Requires predefined *k*<br>❌ Sensitive to initialization<br>❌ Assumes spherical clusters | Homogeneous landscapes with clear spectral separation |\n","| **MiniBatchKMeans** | ✅ Faster than KMeans for large datasets<br>✅ Memory-efficient       | ❌ Slightly lower accuracy<br>❌ Same limitations as KMeans          | Large raster stacks (>1M pixels)          |\n","| **DBSCAN**          | ✅ No need to specify *k*<br>✅ Handles irregular shapes<br>✅ Identifies noise | ❌ Sensitive to parameters (*eps*, *min_samples*)<br>❌ Struggles with varying densities | Heterogeneous landscapes with natural clusters |\n","| **Agglomerative**   | ✅ Hierarchical structure<br>✅ Flexible distance metrics<br>✅ Visualizable dendrograms | ❌ Computationally expensive (O(n³))<br>❌ Memory-intensive | Small datasets with hierarchical patterns |\n","| **Spectral**        | ✅ Captures complex structures<br>✅ Works with non-convex shapes     | ❌ Requires affinity matrix (memory-heavy)<br>❌ Slow for large datasets | High-dimensional data with non-linear patterns |\n","| **GMM**             | ✅ Soft clustering (probabilities)<br>✅ Flexible cluster shapes      | ❌ Sensitive to initialization<br>❌ Assumes Gaussian distributions  | Probabilistic classification needs        |\n"],"metadata":{"id":"aPZJT048vsUU"}},{"cell_type":"markdown","source":["### **Key Evaluation Metrics**\n","| Metric               | Range          | Interpretation                          |\n","|----------------------|----------------|-----------------------------------------|\n","| **Silhouette Score** | -1 to 1        | Higher = Better separation              |\n","| **Calinski-Harabasz**| 0 to ∞         | Higher = Better defined clusters        |\n","| **Davies-Bouldin**   | 0 to ∞         | Lower = Better cluster separation       |"],"metadata":{"id":"orRg2mHcxs2D"}},{"cell_type":"markdown","source":["\n","*Practical Recommendations*\n","\n","For large datasets:\n","\n","* Start with MiniBatchKMeans (speed) or HDBSCAN (auto-clustering)\n","\n","For noisy data:\n","\n","* Use DBSCAN or HDBSCAN to filter noise\n","\n","For hierarchical patterns:\n","\n","* Agglomerative Clustering with linkage plots\n","\n","When k is unknown:\n","\n","* Run KMeans with elbow/silhouette analysis first\n","\n","For spatial coherence:\n","\n","* Add XY coordinates as features or use SpatialKMeans"],"metadata":{"id":"cIOqJslev6jl"}},{"cell_type":"markdown","source":["## Visualization Workflow"],"metadata":{"id":"ry3wMhT8wVw2"}},{"cell_type":"code","source":["# PCA Scatter Plot:\n","plt.scatter(X_pca[:,0], X_pca[:,1], c=labels, cmap='viridis', s=1)"],"metadata":{"id":"TtUMZ27Ku9ZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cluster map:\n","plt.imshow(labels.reshape(ds.y.size, ds.x.size), cmap='tab20')"],"metadata":{"id":"oa4A3wLhwcce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dendrogram:\n","from scipy.cluster.hierarchy import dendrogram\n","dendrogram(model.children_, truncate_mode='level', p=3)"],"metadata":{"id":"PyAhFJvwwhgp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Performance Benchmark Example**\n","| Model               | Silhouette | Calinski-Harabasz | Davies-Bouldin | Time (s) |\n","|---------------------|------------|-------------------|----------------|----------|\n","| KMeans (k=5)        | 0.62       | 1204              | 0.81           | 12.1     |\n","| DBSCAN (eps=0.3)    | 0.58       | 984               | 0.92           | 8.7      |\n","| Spectral (k=5)      | 0.65       | 1350              | 0.75           | 42.3     |\n","\n","\n","Trade-off: Spectral clustering often performs best but is 3-4x slower than KMeans. For time-critical applications, MiniBatchKMeans provides a good balance."],"metadata":{"id":"l5pTMTiHwt9M"}},{"cell_type":"markdown","source":["### **Recommendations**\n","1. **For computational efficiency**: `MiniBatchKMeans`  \n","2. **For automatic cluster detection**: `DBSCAN`/`HDBSCAN`  \n","3. **For hierarchical relationships**: `Agglomerative` + dendrogram  \n","4. **For spatial coherence**: Include XY coordinates as features"],"metadata":{"id":"YviQxBx9x-09"}},{"cell_type":"markdown","source":["### Save and predict\n"],"metadata":{"id":"GouUtP0WLtdp"}},{"cell_type":"code","source":["# Training Phase (Reference Year)\n","import numpy as np\n","import xarray as xr\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import StandardScaler\n","import joblib  # for model saving\n","\n","# Load reference year data\n","ds_train = xr.open_dataset('2018_raster.nc')  # Your training year\n","X_train = ds_train.to_array().values.reshape(ds_train.dims['variable'], -1).T\n","\n","# Preprocess\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","\n","# Train model\n","kmeans = KMeans(n_clusters=5, random_state=42)\n","kmeans.fit(X_train_scaled)\n","\n","# Save artifacts\n","joblib.dump(kmeans, 'kmeans_model.pkl')\n","joblib.dump(scaler, 'scaler.pkl')"],"metadata":{"id":"YoilLMwhL47P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediction Phase (New Year)\n","# Load new year data\n","ds_test = xr.open_dataset('2022_raster.nc')  # New year to classify\n","X_test = ds_test.to_array().values.reshape(ds_test.dims['variable'], -1).T\n","\n","# Load saved models\n","kmeans = joblib.load('kmeans_model.pkl')\n","scaler = joblib.load('scaler.pkl')\n","\n","# Predict clusters\n","X_test_scaled = scaler.transform(X_test)  # Use same scaling!\n","test_labels = kmeans.predict(X_test_scaled)\n","\n","# Reshape to original raster\n","cluster_map = test_labels.reshape(ds_test.dims['y'], ds_test.dims['x'])"],"metadata":{"id":"pB-Cp0AiL-Nd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Key Considerations*\n","- Feature Alignment:\tEnsure same bands/variables in same order, Unsupervised models don't generalize to new features.\n","- Scaling:\tUse the same scaler from training to prevent cluster distortion from different value ranges.\n","- Dimensionality:\tApply identical PCA if used during training. Maintains the same feature space.\n","- Cluster Interpretation:\tCompare class statistics between years to verify ecological consistency of clusters.\n"],"metadata":{"id":"100FFCRhMRqM"}},{"cell_type":"markdown","source":["### Alternative Approaches"],"metadata":{"id":"CI7p4ZW8M54Y"}},{"cell_type":"code","source":["\n","# A. Direct Transfer (Best for stable environments)\n","## Simply apply the trained model to new data\n","labels_new = kmeans.predict(new_data_scaled)\n","\n","# B. Fine-Tuning (For gradual changes)\n","## Use previous clusters as initialization\n","kmeans_new = KMeans(n_clusters=5, init=kmeans.cluster_centers_, n_init=1)\n","kmeans_new.fit(new_data_scaled)\n","\n","# C. Pseudo-Labeling (When some ground truth exists)\n","from sklearn.semi_supervised import SelfTrainingClassifier\n","base_model = KMeans(n_clusters=5)\n","st_classifier = SelfTrainingClassifier(base_model)\n","st_classifier.fit(partial_labeled_data)"],"metadata":{"id":"FEIdRziBMPK5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Validation Methods:"],"metadata":{"id":"CKoVdGtANPIj"}},{"cell_type":"code","source":["#Spectral Signature Comparison\n","# Compare mean band values per cluster between years\n","pd.DataFrame({\n","    '2018': ds_train.groupby(cluster_map_train).mean(),\n","    '2022': ds_test.groupby(cluster_map_test).mean()\n","})"],"metadata":{"id":"RFm08DXmNSFx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Spatial Consistency Check\n","import matplotlib.pyplot as plt\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n","ax1.imshow(cluster_map_train, cmap='tab20')\n","ax2.imshow(cluster_map_test, cmap='tab20')"],"metadata":{"id":"hTAhYAqyNXtd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Change Detection\n","from sklearn.metrics import confusion_matrix\n","cm = confusion_matrix(cluster_map_train.flatten(), cluster_map_test.flatten())\n","sns.heatmap(cm, annot=True)"],"metadata":{"id":"eg_Ym2s9NccN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**When This Works Best**\n","- Temporal consistency: Similar phenology/conditions between years\n","\n","- Stable features: Same sensors/bands with consistent radiometry\n","\n","- Moderate changes: Gradual land cover evolution rather than abrupt changes\n","\n","For radically different conditions (e.g., wildfire year), consider retraining or using ensemble methods. The approach works well for applications like:\n","\n","- Annual crop type mapping\n","\n","- Urban expansion monitoring\n","\n","- Seasonal vegetation patterns\n","\n","[Source](https://https://chat.deepseek.com/a/chat/s/b31ae746-5a2a-4a17-800f-2c162618c5e6)"],"metadata":{"id":"it562qt-NekI"}},{"cell_type":"markdown","source":["# Unsupervised Classification of Raster Stacks V2"],"metadata":{"id":"g-6rWDb6GE3e"}},{"cell_type":"markdown","source":["| Model                            | Type          | Description                                      |\n","| -------------------------------- | ------------- | ------------------------------------------------ |\n","| **KMeans**                       | Centroid      | Assigns each point to the nearest cluster center |\n","| **Gaussian Mixture Model (GMM)** | Probabilistic | Models data as mixtures of Gaussians             |\n","| **Spectral Clustering**          | Graph-based   | Uses eigenvectors of similarity matrix           |\n","| **DBSCAN**                       | Density       | Groups based on density, robust to noise         |\n","| **Self-Organizing Map (SOM)**    | Neural        | Neural network mapping to a low-dimensional grid |\n","| **Autoencoder + KMeans**         | Deep Hybrid   | Compress data, then cluster in latent space      |\n"],"metadata":{"id":"3obo6EbNGNou"}},{"cell_type":"markdown","source":["## Preprocessing\n","\n"],"metadata":{"id":"uU1a9WH1GQ2A"}},{"cell_type":"code","source":["import xarray as xr\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","\n","# Convert to DataFrame\n","ds = xr.open_dataset(\"your_stack.nc\")\n","arr = ds.to_array().values  # (bands, y, x)\n","arr_2d = arr.reshape(arr.shape[0], -1).T  # (pixels, bands)\n","\n","# Normalize\n","scaler = StandardScaler()\n","X = scaler.fit_transform(arr_2d)\n"],"metadata":{"id":"Tv1OQoYxwtPc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clustering Options"],"metadata":{"id":"3vb_YUZ6GYQL"}},{"cell_type":"markdown","source":["### A. K-means"],"metadata":{"id":"IW40wlc8Gco9"}},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","\n","kmeans = KMeans(n_clusters=5, random_state=0)\n","labels_kmeans = kmeans.fit_predict(X)\n"],"metadata":{"id":"NCiFLLayGY7m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### B. Gaussian Mixture Model"],"metadata":{"id":"9pdpA1MwGfN5"}},{"cell_type":"code","source":["from sklearn.mixture import GaussianMixture\n","\n","gmm = GaussianMixture(n_components=5, covariance_type='full', random_state=0)\n","labels_gmm = gmm.fit_predict(X)\n"],"metadata":{"id":"4Vq_6-o8GhCu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### C. DBSCAN (with tuned eps)"],"metadata":{"id":"89H_Qt0gGlDf"}},{"cell_type":"code","source":["from sklearn.cluster import DBSCAN\n","\n","dbscan = DBSCAN(eps=0.5, min_samples=10)\n","labels_dbscan = dbscan.fit_predict(X)\n"],"metadata":{"id":"Tjo7CvG2Glqd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["D. Autoencoder + KMeans (optional)\n","\n","Use tensorflow.keras to reduce dimensionality, then apply KMeans."],"metadata":{"id":"lBQ_K9riGp63"}},{"cell_type":"markdown","source":["## Evaluation Metrics (Unsupervised)\n","\n","Since there's no ground truth, we rely on intrinsic clustering quality metrics:"],"metadata":{"id":"7YxEWtH4GwCk"}},{"cell_type":"code","source":["from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","def evaluate(X, labels):\n","    print(\"Silhouette Score:\", silhouette_score(X, labels))\n","    print(\"Calinski-Harabasz Score:\", calinski_harabasz_score(X, labels))\n","    print(\"Davies-Bouldin Score:\", davies_bouldin_score(X, labels))\n"],"metadata":{"id":"5S6-HeLzGqjw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plotting\n","### Reshape back to image"],"metadata":{"id":"RSSNnln8G1bE"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","height, width = arr.shape[1:]\n","labels_img = labels_kmeans.reshape(height, width)\n","\n","plt.imshow(labels_img, cmap=\"tab10\")\n","plt.title(\"KMeans Classification\")\n","plt.axis('off')\n","plt.show()\n"],"metadata":{"id":"KmszjEX-G2hZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### PCA Plot"],"metadata":{"id":"AB6OK0RzG9Ot"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","import seaborn as sns\n","import pandas as pd\n","\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X)\n","\n","df_plot = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n","df_plot[\"Cluster\"] = labels_kmeans\n","\n","sns.scatterplot(data=df_plot, x=\"PC1\", y=\"PC2\", hue=\"Cluster\", palette=\"tab10\")\n","plt.title(\"Cluster Visualization (PCA)\")\n","plt.show()\n"],"metadata":{"id":"e08VwbfpG9z_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Summary of Models"],"metadata":{"id":"dRbdDyCPHA7P"}},{"cell_type":"markdown","source":["| Model                   | Pros                                     | Cons                                                   |\n","| ----------------------- | ---------------------------------------- | ------------------------------------------------------ |\n","| **KMeans**              | Fast, easy to implement                  | Assumes spherical clusters, sensitive to `k`           |\n","| **GMM**                 | Probabilistic, handles elliptical shapes | Computationally expensive, sensitive to initialization |\n","| **DBSCAN**              | No need for `k`, detects noise           | Poor in high-dimensions, sensitive to `eps`            |\n","| **Spectral Clustering** | Good with non-convex clusters            | Slow on large datasets                                 |\n","| **SOM**                 | Interpretable, maps to grid              | Needs careful tuning, less common                      |\n","| **Autoencoder+KMeans**  | Learns latent structure, robust          | Complex to train, risk of overfitting                  |\n","\n"],"metadata":{"id":"UJriBl35HEFI"}},{"cell_type":"markdown","source":["### Recommendation\n","\n","\n","If your raster stack is moderately sized, try:\n","\n","- KMeans and GMM for quick and reliable results.\n","\n","- DBSCAN if you expect noise and irregular clusters.\n","\n","- Autoencoder + KMeans if the stack is large or you suspect nonlinear structure.\n","\n","- Use Silhouette Score and Calinski-Harabasz for comparison."],"metadata":{"id":"1zKmw1OgHOVA"}},{"cell_type":"markdown","source":["## Full pipeline"],"metadata":{"id":"AXRU4AxmIUF5"}},{"cell_type":"code","source":["# Unsupervised Classification on Raster Stack using xarray, scikit-learn, and Autoencoder\n","\n","import xarray as xr\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans, DBSCAN\n","from sklearn.mixture import GaussianMixture\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Load Raster Stack (assuming NetCDF format)\n","ds = xr.open_dataset(\"your_raster_stack.nc\")  # Replace with your file path\n","arr = ds.to_array().values  # Shape: (bands, y, x)\n","bands, height, width = arr.shape\n","arr_2d = arr.reshape(bands, -1).T  # Shape: (pixels, bands)\n","\n","# Remove NaNs\n","valid_mask = ~np.any(np.isnan(arr_2d), axis=1)\n","X_valid = arr_2d[valid_mask]\n","\n","# Normalize Features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X_valid)\n","\n","# Function to evaluate clustering\n","\n","def evaluate_clustering(X, labels, name):\n","    print(f\"\\n--- {name} ---\")\n","    print(\"Silhouette Score:\", silhouette_score(X, labels))\n","    print(\"Calinski-Harabasz Score:\", calinski_harabasz_score(X, labels))\n","    print(\"Davies-Bouldin Score:\", davies_bouldin_score(X, labels))\n","\n","# 1. KMeans\n","kmeans = KMeans(n_clusters=5, random_state=42)\n","kmeans_labels = kmeans.fit_predict(X_scaled)\n","evaluate_clustering(X_scaled, kmeans_labels, \"KMeans\")\n","\n","# 2. Gaussian Mixture Model\n","gmm = GaussianMixture(n_components=5, covariance_type='full', random_state=42)\n","gmm_labels = gmm.fit_predict(X_scaled)\n","evaluate_clustering(X_scaled, gmm_labels, \"GMM\")\n","\n","# 3. DBSCAN (parameters may need tuning)\n","dbscan = DBSCAN(eps=0.5, min_samples=10)\n","dbscan_labels = dbscan.fit_predict(X_scaled)\n","\n","# Filter out noise (-1 labels)\n","if len(set(dbscan_labels)) > 1 and -1 in dbscan_labels:\n","    print(\"\\nDBSCAN filtered noise\")\n","    evaluate_clustering(X_scaled[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1], \"DBSCAN\")\n","else:\n","    print(\"\\nDBSCAN did not identify more than one cluster.\")\n","\n","# 4. Autoencoder-based Feature Extraction\n","def build_autoencoder(input_dim):\n","    input_layer = Input(shape=(input_dim,))\n","    encoded = Dense(64, activation='relu')(input_layer)\n","    encoded = Dense(32, activation='relu')(encoded)\n","    encoded = Dense(10, activation='relu')(encoded)\n","    decoded = Dense(32, activation='relu')(encoded)\n","    decoded = Dense(64, activation='relu')(decoded)\n","    output_layer = Dense(input_dim, activation='linear')(decoded)\n","    autoencoder = Model(input_layer, output_layer)\n","    encoder = Model(input_layer, encoded)\n","    return autoencoder, encoder\n","\n","input_dim = X_scaled.shape[1]\n","autoencoder, encoder = build_autoencoder(input_dim)\n","autoencoder.compile(optimizer='adam', loss='mse')\n","\n","early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n","autoencoder.fit(X_scaled, X_scaled, epochs=100, batch_size=256, shuffle=True, callbacks=[early_stop], verbose=0)\n","\n","X_encoded = encoder.predict(X_scaled)\n","\n","# Clustering on Autoencoder-encoded Features\n","auto_kmeans = KMeans(n_clusters=5, random_state=42)\n","auto_kmeans_labels = auto_kmeans.fit_predict(X_encoded)\n","evaluate_clustering(X_encoded, auto_kmeans_labels, \"Autoencoder + KMeans\")\n","\n","# PCA for visualization\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X_scaled)\n","X_enc_pca = pca.fit_transform(X_encoded)\n","\n","df_plot = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n","df_plot[\"KMeans\"] = kmeans_labels\n","df_plot[\"GMM\"] = gmm_labels\n","if len(set(dbscan_labels)) > 1:\n","    df_plot[\"DBSCAN\"] = dbscan_labels\n","df_plot_auto = pd.DataFrame(X_enc_pca, columns=[\"PC1\", \"PC2\"])\n","df_plot_auto[\"Auto+KMeans\"] = auto_kmeans_labels\n","\n","# Plotting clusters\n","plt.figure(figsize=(20, 4))\n","methods = [\"KMeans\", \"GMM\"] + ([\"DBSCAN\"] if \"DBSCAN\" in df_plot else [])\n","for i, method in enumerate(methods):\n","    plt.subplot(1, len(methods) + 1, i + 1)\n","    sns.scatterplot(data=df_plot, x=\"PC1\", y=\"PC2\", hue=method, palette=\"tab10\", s=10, linewidth=0)\n","    plt.title(f\"{method} Clusters\")\n","    plt.legend(loc='best', fontsize='small')\n","\n","plt.subplot(1, len(methods) + 1, len(methods) + 1)\n","sns.scatterplot(data=df_plot_auto, x=\"PC1\", y=\"PC2\", hue=\"Auto+KMeans\", palette=\"tab10\", s=10, linewidth=0)\n","plt.title(\"Autoencoder + KMeans\")\n","plt.legend(loc='best', fontsize='small')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Reconstruct cluster image for visualization (Autoencoder + KMeans)\n","labels_full = np.full(arr_2d.shape[0], -1)\n","labels_full[valid_mask] = auto_kmeans_labels\n","img_auto_kmeans = labels_full.reshape(height, width)\n","\n","plt.imshow(img_auto_kmeans, cmap=\"tab10\")\n","plt.title(\"Autoencoder + KMeans Cluster Map\")\n","plt.axis('off')\n","plt.show()\n","\n"],"metadata":{"id":"OFilSwuRHBrr"},"execution_count":null,"outputs":[]}]}